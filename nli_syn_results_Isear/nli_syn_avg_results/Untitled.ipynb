{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad24a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "data = pd.read_csv('Isear_nli_syn_avg_expr_prompt.csv')\n",
    "#Isear_nli_syn_avg_feels_prompt.csv\n",
    "data = pd.read_csv('Isear_nli_syn_avg_feels_prompt.csv')\n",
    "#data2 = pd.read_csv('TEC2_roberta_nli2.csv')\n",
    "#data3=pd.read_csv('TEC2_roberta_nli2.csv')\n",
    "y_true = data['labels'].tolist()\n",
    "text=data['text'].tolist()\n",
    "#y_pred = data['prob_expr_s'].tolist()\n",
    "probs=data['prob_feels_s'].tolist()\n",
    "max_probs=list()\n",
    "\n",
    "#for m in probs:\n",
    "#    max_probs.append(max(m))\n",
    "\n",
    "for m in probs:\n",
    "    m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "    string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "    float_list = [float(element) for element in string_elements]\n",
    "    \n",
    "#    r=list(m)\n",
    "#    print(m)\n",
    "#    for d in float_list:\n",
    " #       arr = [float(i) for i in d[1:-1].split(\",\")]\n",
    " #       max_index=np.argmax(arr)\n",
    "    max_probs.append(max(float_list))    \n",
    "data['prob_max']=max_probs\n",
    "data.to_csv('Isear_nli_syn_avg_feels_prompt2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9b8fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01796317802699985\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import statistics\n",
    "\n",
    "data = pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "#Isear_nli_syn_avg_emo_s_prompt2.csv  0.034\n",
    "#Isear_nli_syn_avg_emotion_prompt2.csv  0.017\n",
    "var=statistics.variance(data['prob_max'])\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67f02c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame']\n"
     ]
    }
   ],
   "source": [
    "unique_labels = sorted(list(set(label_list)))\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a06454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7215761470220254\n",
      "0.05582110896620998\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import statistics\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#Isear_nli_syn_avg_emo_s_prompt.csv\n",
    "data = pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_avg_emo_s_prompt.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_avg_feels_prompt2.csv')\n",
    "\n",
    "#Isear_nli_syn_avg_expr_prompt2\n",
    "#Isear_nli_syn_avg_emo_s_prompt2.csv  0.034\n",
    "#Isear_nli_syn_avg_emotion_prompt2.csv  0.017\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['feels_s'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_feels_s'].tolist()\n",
    "probs2=list()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_predict[i]!=y_true[i] and y_true[i]=='shame':\n",
    "        m=probs[i]\n",
    "        m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "        string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "        float_list = [float(element) for element in string_elements]\n",
    "        probs2.append(float_list[6])\n",
    "\n",
    "ss=statistics.mean(probs2)\n",
    "var=statistics.variance(probs2)\n",
    "print(ss)\n",
    "print(var)\n",
    "\n",
    "\n",
    "#wrong prediction\n",
    "#emo_s\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'\n",
    "#anger:0.53, 0.47,0.47, 0.56, 0.23,0.45, 0.47\n",
    "#angr:0.02, 0.05\n",
    "\n",
    "#emotion prompt\n",
    "#anger:0.71, disgust:0.62, fear:0.68, guilt:0.75, joy:0.35, sadness:0.6, shame:0.45\n",
    "#anger:0.03, disgust:0.05, fear:0.04, guilt:0.02, joy:0.04, sadness:0.05  ,shame:0.06\n",
    "\n",
    "#expr_s\n",
    "# anger:0.7, disgust:0.65 fear:0.62 guilt:0.71  joy:0.31  sadness:0.64  shame:0.61\n",
    "#0.04  0.06  0.07  0.04  0.05  0.06   0.06\n",
    "\n",
    "#feels_s\n",
    "#anger:0.77 disgust:0.67 fear:0.73 guilt:0.75   sadness:0.75 joy:0.48  shame:0.72\n",
    "# 0.04  , 0.06 0.05  0.04  0.04 0.07 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec6e27a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8412292122522743\n",
      "0.02294825152319945\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import statistics\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#Isear_nli_syn_avg_emo_s_prompt.csv\n",
    "data = pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_emo_s_prompt.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_avg_expr_prompt2.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_avg_feels_prompt2.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_avg_emo_s_prompt2.csv')\n",
    "#Isear_nli_syn_avg_expr_prompt2.csv\n",
    "#Isear_nli_syn_avg_emo_s_prompt2.csv  0.034\n",
    "#Isear_nli_syn_avg_emotion_prompt2.csv  0.017\n",
    "#Isear_nli_syn_expr_emo_promp_max_edited.csv\n",
    "#Isear_nli_syn_avg_feels_prompt2.csv\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['emo_s'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_emo_s'].tolist()\n",
    "probs2=list()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_predict[i]==y_true[i] and y_true[i]=='shame':\n",
    "        m=probs[i]\n",
    "        m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "        string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "        float_list = [float(element) for element in string_elements]\n",
    "        probs2.append(float_list[6])\n",
    "\n",
    "ss=statistics.mean(probs2)\n",
    "var=statistics.variance(probs2)\n",
    "print(ss)\n",
    "print(var)\n",
    "\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'\n",
    "\n",
    "#emo_s\n",
    "#anger:0.81  disgust:0.77  fear:0.76  guilt:0.77 joy:0.72 sandess:0.76  shame:0.84\n",
    "#0.02   0.03  0.03  0.03  0.02  0.03  0.018  0.022\n",
    "\n",
    "#expr_s\n",
    "#anger: 0.92,  disgust:0.91  , fear:0.91 , guilt:0.9  , joy:0.83 , sandess:0.91,  shame:0.93\n",
    "#0.013 0.018 0.011  0.017  0.02  0.011  0.009\n",
    "\n",
    "\n",
    "#emotion_s\n",
    "#anger:0.87 disgust: 0.9  fear:0.9  guilt:0.86   joy: 0.83  sadness:0.88   shame:0.91  \n",
    "# 0.008    0.01      0.01   0.01    0.012   0.008  0.1\n",
    "\n",
    "#feels_s\n",
    "#'anger' 0.92, 'disgust':0.85, 'fear'0.94, 'guilt'0.92, 'joy':0.9, 'sadness'0.93, 'shame' 0.96\n",
    "#0.01  0.017  0.006  0.012  0.009  0.006  0.006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9d95dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7897746075121731\n",
      "0.031739802589780854\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import statistics\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#Isear_nli_syn_avg_emo_s_prompt.csv\n",
    "data = pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_emo_s_prompt.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_feels_prompt2.csv')\n",
    "#Isear_nli_syn_avg_expr_prompt2.csv\n",
    "#Isear_nli_syn_avg_emo_s_prompt2.csv  0.034\n",
    "#Isear_nli_syn_avg_emotion_prompt2.csv  0.017\n",
    "#Isear_nli_syn_expr_emo_promp_max_edited.csv\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['emotion'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_emotion'].tolist()\n",
    "probs2=list()\n",
    "\n",
    "unique_labels = sorted(list(set(label_list)))\n",
    "print(unique_labels)\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i]=='anger':\n",
    "        m=probs[i]\n",
    "        m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "        string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "        float_list = [float(element) for element in string_elements]\n",
    "        probs2.append(float_list[0])\n",
    "\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'        \n",
    "ss=statistics.mean(probs2)\n",
    "var=statistics.variance(probs2)\n",
    "print(ss)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4360180c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame']\n",
      "0.8790193123411513\n",
      "0.008569364215044034\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import statistics\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#Isear_nli_syn_avg_emo_s_prompt.csv\n",
    "data = pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_emo_s_prompt.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_feels_prompt2.csv')\n",
    "#Isear_nli_syn_avg_expr_prompt2.csv\n",
    "#Isear_nli_syn_avg_emo_s_prompt2.csv  0.034\n",
    "#Isear_nli_syn_avg_emotion_prompt2.csv  0.017\n",
    "#Isear_nli_syn_expr_emo_promp_max_edited.csv\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['emotion'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_emotion'].tolist()\n",
    "probs2=list()\n",
    "\n",
    "unique_labels = sorted(list(set(y_true)))\n",
    "print(unique_labels)\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i]=='anger' and y_predict[i]=='anger':\n",
    "        m=probs[i]\n",
    "        m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "        string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "        float_list = [float(element) for element in string_elements]\n",
    "        probs2.append(float_list[0])\n",
    "\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'        \n",
    "ss=statistics.mean(probs2)\n",
    "var=statistics.variance(probs2)\n",
    "print(ss)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c365075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[503  40  45 258   9  53 171]\n",
      " [157 415 119 191  24  38 122]\n",
      " [ 43  10 843 103  12  45  20]\n",
      " [ 98  17  28 719   9  62 116]\n",
      " [  4   1  21  49 994  17   6]\n",
      " [ 40  13  59 101  15 774  80]\n",
      " [ 63  27  59 347  26  39 510]]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import statistics\n",
    "\n",
    "data = pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_expr_prompt2.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_emo_s_prompt.csv')\n",
    "##data=pd.read_csv('Isear_nli_syn_avg_feels_prompt2.csv')\n",
    "\n",
    "l=data['labels']\n",
    "p=data['emotion']\n",
    "#print(confusion_matrix(l, p))\n",
    "print(classification_report(l, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ece601d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "data = pd.read_csv('Isear_nli_syn_avg_expr_prompt2.csv')\n",
    "\n",
    "###\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['expr_s'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_expr_s'].tolist()\n",
    "prob_max=data['prob_max'].tolist()\n",
    "probs2=list()\n",
    "entropy_list=list()\n",
    "##\n",
    "labels2=list()\n",
    "expr_s2=list()\n",
    "text2=list()\n",
    "prob_max2=list()\n",
    "prob2=list()\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_predict[i]!=y_true[i] and y_true[i]=='sadness':\n",
    "        m=probs[i]\n",
    "        m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "        string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "        float_list = [float(element) for element in string_elements]\n",
    "        \n",
    "        e=entropy(float_list, base=2)\n",
    "        entropy_list.append(e)\n",
    "        ##\n",
    "        labels2.append(y_true[i])\n",
    "        expr_s2.append(y_predict[i])\n",
    "        text2.append(text[i])\n",
    "        prob2.append(probs[i])\n",
    "        prob_max2.append(prob_max[i])\n",
    "\n",
    "data2={'labels':labels2, 'text':text2, 'expr_s':expr_s2, 'prob_expr_s':prob2, 'prob_max':prob_max2, 'entropy':entropy_list}\n",
    "d=pd.DataFrame(data2)\n",
    "\n",
    "d.to_csv('Isear_nli_syn_avg_expr_prompt3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84d74780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "data = pd.read_csv('Isear_nli_syn_avg_expr_prompt2.csv')\n",
    "\n",
    "###\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['expr_s'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_expr_s'].tolist()\n",
    "prob_max=data['prob_max'].tolist()\n",
    "probs2=list()\n",
    "entropy_list=list()\n",
    "##\n",
    "labels2=list()\n",
    "expr_s2=list()\n",
    "text2=list()\n",
    "prob_max2=list()\n",
    "prob2=list()\n",
    "var_list=list()\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "#    if y_predict[i]!=y_true[i] and y_true[i]=='sadness':\n",
    "    m=probs[i]\n",
    "    m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "    string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "    float_list = [float(element) for element in string_elements]\n",
    "        \n",
    "#        e=entropy(float_list, base=2)\n",
    "        \n",
    "    var=statistics.variance(float_list)\n",
    "    var_list.append(var)\n",
    "#    entropy_list.append(e)\n",
    "        ##\n",
    "    labels2.append(y_true[i])\n",
    "    expr_s2.append(y_predict[i])\n",
    "    text2.append(text[i])\n",
    "    prob2.append(probs[i])\n",
    "    prob_max2.append(prob_max[i])\n",
    "\n",
    "data2={'labels':labels2, 'text':text2, 'expr_s':expr_s2, 'prob_expr_s':prob2, 'prob_max':prob_max2, 'variance':var_list}\n",
    "d=pd.DataFrame(data2)\n",
    "\n",
    "d.to_csv('Isear_nli_syn_avg_expr_prompt2_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "845919da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "data = pd.read_csv('Isear_nli_syn_avg_expr_prompt2.csv')\n",
    "#Isear_nli_syn_avg_emo_s_prompt2.csv\n",
    "data = pd.read_csv('Isear_nli_syn_avg_emo_s_prompt2.csv')\n",
    "\n",
    "###\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['emo_s'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_emo_s'].tolist()\n",
    "prob_max=data['prob_max'].tolist()\n",
    "probs2=list()\n",
    "entropy_list=list()\n",
    "##\n",
    "labels2=list()\n",
    "expr_s2=list()\n",
    "text2=list()\n",
    "prob_max2=list()\n",
    "prob2=list()\n",
    "var_list=list()\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "#    if y_predict[i]!=y_true[i] and y_true[i]=='sadness':\n",
    "    m=probs[i]\n",
    "    m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "    string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "    float_list = [float(element) for element in string_elements]\n",
    "        \n",
    "#        e=entropy(float_list, base=2)\n",
    "        \n",
    "    var=statistics.variance(float_list)\n",
    "    var_list.append(var)\n",
    "#    entropy_list.append(e)\n",
    "        ##\n",
    "    labels2.append(y_true[i])\n",
    "    expr_s2.append(y_predict[i])\n",
    "    text2.append(text[i])\n",
    "    prob2.append(probs[i])\n",
    "    prob_max2.append(prob_max[i])\n",
    "\n",
    "data2={'labels':labels2, 'text':text2, 'emo_s':expr_s2, 'prob_emo_s':prob2, 'prob_max':prob_max2, 'variance':var_list}\n",
    "d=pd.DataFrame(data2)\n",
    "\n",
    "d.to_csv('Isear_nli_syn_avg_emo_s_prompt2_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac48cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "data = pd.read_csv('Isear_nli_syn_avg_expr_prompt2.csv')\n",
    "#Isear_nli_syn_avg_emo_s_prompt2.csv\n",
    "#Isear_nli_syn_avg_expr_prompt2.csv\n",
    "\n",
    "###\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['expr_s'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_expr_s'].tolist()\n",
    "prob_max=data['prob_max'].tolist()\n",
    "probs2=list()\n",
    "entropy_list=list()\n",
    "##\n",
    "labels2=list()\n",
    "expr_s2=list()\n",
    "text2=list()\n",
    "prob_max2=list()\n",
    "prob2=list()\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_predict[i]!=y_true[i] and y_true[i]=='sadness':\n",
    "        m=probs[i]\n",
    "        m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "        string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "        float_list = [float(element) for element in string_elements]\n",
    "        \n",
    "        e=entropy(float_list, base=2)\n",
    "        entropy_list.append(e)\n",
    "        ##\n",
    "        labels2.append(y_true[i])\n",
    "        expr_s2.append(y_predict[i])\n",
    "        text2.append(text[i])\n",
    "        prob2.append(probs[i])\n",
    "        prob_max2.append(prob_max[i])\n",
    "\n",
    "data2={'labels':labels2, 'text':text2, 'expr_s':expr_s2, 'prob_expr_s':prob2, 'prob_max':prob_max2, 'entropy':entropy_list}\n",
    "d=pd.DataFrame(data2)\n",
    "\n",
    "d.to_csv('Isear_nli_syn_avg_expr_prompt3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e313d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "data = pd.read_csv('Isear_nli_syn_avg_emo_s_prompt2.csv')\n",
    "#Isear_nli_syn_avg_emo_s_prompt2\n",
    "###\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['emo_s'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_emo_s'].tolist()\n",
    "prob_max=data['prob_max'].tolist()\n",
    "probs2=list()\n",
    "entropy_list=list()\n",
    "##\n",
    "labels2=list()\n",
    "expr_s2=list()\n",
    "text2=list()\n",
    "prob_max2=list()\n",
    "prob2=list()\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    \n",
    "    m=probs[i]\n",
    "    m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "    string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "    float_list = [float(element) for element in string_elements]\n",
    "    float_list2=[(i)/max(float_list) for i in float_list]\n",
    "    \n",
    "        \n",
    "    e=entropy(float_list2, base=2)\n",
    "    \n",
    "    entropy_list.append(e)\n",
    "        ##\n",
    "    labels2.append(y_true[i])\n",
    "    expr_s2.append(y_predict[i])\n",
    "    text2.append(text[i])\n",
    "    prob2.append(probs[i])\n",
    "    prob_max2.append(prob_max[i])\n",
    "\n",
    "data2={'labels':labels2, 'text':text2, 'emo_s':expr_s2, 'prob_emo_s':prob2, 'prob_max':prob_max2, 'entropy':entropy_list}\n",
    "d=pd.DataFrame(data2)\n",
    "\n",
    "d.to_csv('Isear_nli_syn_avg_emo_prompt3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "697ce3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5486001700754906\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "\n",
    "data = pd.read_csv('Isear_nli_syn_avg_emo_s_prompt3.csv')\n",
    "y_true = data['labels'].tolist()\n",
    "entropy_list=data['entropy'].tolist()\n",
    "emo_s=data['emo_s'].tolist()\n",
    "entropy_list2=list()\n",
    "\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'  \n",
    "\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i]=='guilt' and emo_s[i]!='guilt':\n",
    "        entropy_list2.append(entropy_list[i])\n",
    "mean=statistics.mean(entropy_list2)\n",
    "print(mean) \n",
    "\n",
    "#True\n",
    "#anger:2.54, disgust:2.582 fear:2.53 guilt:2.55\n",
    "\n",
    "#wrong\n",
    "#anger:2.58  disgust:2.586 fear:2.59 2.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3bd0c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2953833108846884\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_counts=np.bincount(y_predicimport argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame' \n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "#Isear_nli_syn_avg_expr_prompt2\n",
    "data = pd.read_csv('Isear_nli_syn_avg_emo_s_prompt2.csv')\n",
    "data = pd.read_csv('Isear_nli_syn_avg_expr_prompt2.csv')\n",
    "dict_emo={'anger':0, 'disgust':1, 'fear':2, 'guilt':3, 'joy':4, 'sadness':5, 'shame':6 }\n",
    "#Isear_nli_syn_avg_emo_s_prompt2\n",
    "###\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['expr_s'].tolist()\n",
    "probs=data['prob_expr_s'].tolist()\n",
    "y_predict2=list()\n",
    "probs2=list()\n",
    "emotions=list()\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_predict[i]!=y_true[i] and y_true[i]=='sadness':\n",
    "        y_predict2.append(dict_emo[y_predict[i]])\n",
    "        \n",
    "probs=class_counts/len(y_predict2)\n",
    "entropy_emotions=entropy(probs, base=2)\n",
    "print(entropy_emotions)\n",
    "\n",
    "#entropy, emo_s\n",
    "#anger:1.62 disgust:2.1 fear:1.9 guilt:2.08 joy:1.77 sadness:2.05 shame:1.44\n",
    "\n",
    "#entropy, expr_s\n",
    "#anger:2.29    disgust: 2.26   fear:2.28  guilt:2.28  joy: 2.08  sadness:2.29   shame: 1.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c5f30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "41\n",
      "107\n",
      "171\n",
      "10\n",
      "87\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame' \n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "#Isear_nli_syn_avg_expr_prompt2\n",
    "data = pd.read_csv('Isear_nli_syn_avg_emo_s_prompt2.csv')\n",
    "data = pd.read_csv('Isear_nli_syn_avg_expr_prompt2.csv')\n",
    "dict_emo={'anger':0, 'disgust':1, 'fear':2, 'guilt':3, 'joy':4, 'sadness':5, 'shame':6 }\n",
    "anger=0\n",
    "disgust=0\n",
    "fear=0 \n",
    "guilt=0 \n",
    "joy=0\n",
    "sadness=0\n",
    "shame=0 \n",
    "#Isear_nli_syn_avg_emo_s_prompt2\n",
    "###\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['expr_s'].tolist()\n",
    "probs=data['prob_expr_s'].tolist()\n",
    "y_predict2=list()\n",
    "probs2=list()\n",
    "emotions=list()\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_predict[i]!=y_true[i] and y_true[i]=='anger':\n",
    "        if y_predict[i]=='anger':\n",
    "            anger+=1\n",
    "        if y_predict[i]=='disgust':\n",
    "            disgust+=1\n",
    "        if y_predict[i]=='fear':\n",
    "            fear+=1\n",
    "        if y_predict[i]=='guilt':\n",
    "            guilt+=1\n",
    "        if y_predict[i]=='joy':\n",
    "            joy+=1\n",
    "        if y_predict[i]=='sadness':\n",
    "            sadness+=1\n",
    "        if y_predict[i]=='shame':\n",
    "            shame+=1\n",
    "print(anger)\n",
    "print(disgust)\n",
    "print(fear)\n",
    "print(guilt)\n",
    "print(joy)\n",
    "print(sadness)\n",
    "print(shame)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7163cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import statistics\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#Isear_nli_syn_avg_emo_s_prompt.csv\n",
    "#data = pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_max_emotion_prompt.csv')\n",
    "#Isear_nli_syn_avg_emotion_prompt2\n",
    "data=pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "\n",
    "l=data['labels'].tolist()\n",
    "e=data['emotion'].tolist()\n",
    "anger=0\n",
    "disgust=0\n",
    "fear=0\n",
    "guilt=0\n",
    "joy=0\n",
    "sadness=0\n",
    "shame=0\n",
    "\n",
    "for i in range(len(l)):\n",
    "    if l[i]=='shame':\n",
    "        if e[i]=='anger':\n",
    "            anger+=1\n",
    "        if e[i]=='disgust':\n",
    "            disgust+=1\n",
    "        if e[i]=='fear':\n",
    "            fear+=1\n",
    "        if e[i]=='guilt':\n",
    "            guilt+=1\n",
    "        if e[i]=='joy':\n",
    "            joy+=1\n",
    "        if e[i]=='sadness':\n",
    "            sadness+=1\n",
    "        if e[i]=='shame':\n",
    "            shame+=1\n",
    "print(anger)\n",
    "print(disgust)\n",
    "print(fear)\n",
    "print(guilt)\n",
    "print(sadness)\n",
    "print(shame)\n",
    "print(joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca498f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
