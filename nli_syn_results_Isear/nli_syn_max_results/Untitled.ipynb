{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff1afb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.45      0.52      0.48      1079\n",
      "     disgust       0.55      0.42      0.48      1066\n",
      "        fear       0.75      0.70      0.72      1076\n",
      "       guilt       0.45      0.61      0.52      1049\n",
      "         joy       0.83      0.90      0.86      1092\n",
      "     sadness       0.54      0.76      0.63      1082\n",
      "       shame       0.76      0.23      0.35      1071\n",
      "\n",
      "    accuracy                           0.59      7515\n",
      "   macro avg       0.62      0.59      0.58      7515\n",
      "weighted avg       0.62      0.59      0.58      7515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import statistics\n",
    "\n",
    "data = pd.read_csv('Isear_nli_syn_max_emo_s_prompt.csv')\n",
    "data = pd.read_csv('Isear_nli_syn_emotion_prompt._max_entailment_Deberta.csv')\n",
    "data = pd.read_csv('Isear_nli_syn_expr_emo_promp_max_edited.csv')\n",
    "data = pd.read_csv('Isear_nli_syn_feels_s_prompt.csv')\n",
    "#Isear_nli_syn_expr_emo_promp_max_edited\n",
    "#Isear_nli_syn_expr_emo_promp_max_edited.csv\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_expr_prompt2.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_emo_s_prompt.csv')\n",
    "##data=pd.read_csv('Isear_nli_syn_avg_feels_prompt2.csv')\n",
    "\n",
    "l=data['labels']\n",
    "p=data['feels_s']\n",
    "print(classification_report(l, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f2d0ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8207868447495394\n",
      "0.03380183538800963\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import statistics\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#Isear_nli_syn_avg_emo_s_prompt.csv\n",
    "#data = pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_max_emo_s_prompt.csv')\n",
    "\n",
    "data=pd.read_csv('Isear_nli_syn_expr_emo_max.csv')\n",
    "#Isear_nli_syn_emotion_prompt_max_entailment_Deberta\n",
    "data=pd.read_csv('Isear_nli_syn_emotion_prompt_max_entailment_Deberta.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_feels_s_prompt_max.csv')\n",
    "#Isear_nli_syn_avg_expr_prompt2\n",
    "#Isear_nli_syn_avg_emo_s_prompt2.csv  0.034\n",
    "#Isear_nli_syn_avg_emotion_prompt2.csv  0.017\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['emotion'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_emotion'].tolist()\n",
    "probs2=list()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_predict[i]!=y_true[i] and y_true[i]=='shame':\n",
    "        m=probs[i]\n",
    "        m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "        string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "        float_list = [float(element) for element in string_elements]\n",
    "        probs2.append(float_list[6])\n",
    "\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'\n",
    "        \n",
    "ss=statistics.mean(probs2)\n",
    "var=statistics.variance(probs2)\n",
    "print(ss)\n",
    "print(var)\n",
    "\n",
    "#emo_s\n",
    "#anger:0.79  disgust:0.73  fear:0.74 guilt: 0.78 joy:0.58  sadness:0.72 shame:0.67\n",
    "#0.02  0.05   0.03   0.03  0.06  0.05  0.05\n",
    "\n",
    "#expr_s\n",
    "#anger: 0.92  disgust 0.88 fear:0.87   guilt:0.89       joy 0.64        sadness 0.85   shame:0.83\n",
    "# 0.02  0.04  0.03   0.03  0.08 0.053 0.05\n",
    "\n",
    "#feels_s\n",
    "#'anger' 0.93, 'disgust' 0.84 , 'fear' 0.9, 'guilt':0.91, 'joy' 0.76, 'sadness' 0.9, 'shame'0.86\n",
    "#0.015  0.05   0.02   0.016  0.07  0.02 0.03\n",
    "\n",
    "#emotion prompt\n",
    "#0.88  0.81  0.85  0.89   0.72 0.8  0.82\n",
    "#0.02  0.05   0.02  0.016  0.06 0.04 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d7d891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4\n",
      "27\n",
      "138\n",
      "22\n",
      "4\n",
      "891\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import statistics\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#Isear_nli_syn_avg_emo_s_prompt.csv\n",
    "#data = pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_max_emotion_prompt.csv')\n",
    "\n",
    "l=data['labels'].tolist()\n",
    "e=data['emotion'].tolist()\n",
    "anger=0\n",
    "disgust=0\n",
    "fear=0\n",
    "guilt=0\n",
    "joy=0\n",
    "sadness=0\n",
    "shame=0\n",
    "\n",
    "for i in range(len(l)):\n",
    "    if l[i]=='joy':\n",
    "        if e[i]=='anger':\n",
    "            anger+=1\n",
    "        if e[i]=='disgust':\n",
    "            disgust+=1\n",
    "        if e[i]=='fear':\n",
    "            fear+=1\n",
    "        if e[i]=='guilt':\n",
    "            guilt+=1\n",
    "        if e[i]=='joy':\n",
    "            joy+=1\n",
    "        if e[i]=='sadness':\n",
    "            sadness+=1\n",
    "        if e[i]=='shame':\n",
    "            shame+=1\n",
    "print(anger)\n",
    "print(disgust)\n",
    "print(fear)\n",
    "print(guilt)\n",
    "print(sadness)\n",
    "print(shame)\n",
    "print(joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa725dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.974087951295187\n",
      "0.00645052574833629\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import statistics\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#Isear_nli_syn_avg_emo_s_prompt.csv\n",
    "#data = pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_avg_emotion_prompt2.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_max_emo_s_prompt.csv')\n",
    "#Isear_nli_syn_expr_emo_max\n",
    "data=pd.read_csv('Isear_nli_syn_expr_emo_max.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_feels_s_prompt_max.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_emotion_prompt_max_entailment_Deberta.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_expr_emo_max.csv')\n",
    "#Isear_nli_syn_emotion_prompt_max_entailment_Deberta\n",
    "#Isear_nli_syn_avg_expr_prompt2\n",
    "#Isear_nli_syn_avg_emo_s_prompt2.csv  0.034\n",
    "#Isear_nli_syn_avg_emotion_prompt2.csv  0.017\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['expr_s'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_expr_s'].tolist()\n",
    "probs2=list()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_predict[i]==y_true[i] and y_true[i]=='shame':\n",
    "        m=probs[i]\n",
    "        m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "        string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "        float_list = [float(element) for element in string_elements]\n",
    "        probs2.append(float_list[6])\n",
    "\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'\n",
    "        \n",
    "ss=statistics.mean(probs2)\n",
    "var=statistics.variance(probs2)\n",
    "print(ss)\n",
    "print(var)\n",
    "\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'\n",
    "\n",
    "#emo_s\n",
    "#anger: 0.94  disgust:0.93  fear:0.93  guilt:0.93   joy:0.92  sadness:0.93  shame:0.96\n",
    "#0.005     0.008            0.01        0.009         0.012   0.005           0.006\n",
    "\n",
    "#expr_s\n",
    "#anger:0.98 disgust 0.097 fear:0.98  0.98  0.96  0.98  0.98\n",
    "#0.001   0.006   0.001  0.002  0.007  0.001  0.002  \n",
    "\n",
    "#feels_s\n",
    "#anger:0.98  disgust:0.97  fear:0.98  guilt:0.98  joy:0.98   sadness:0.98 shame:0.99\n",
    "#0.002 0.004 0.014  0.001  0.002  0.0015  0.0005        0.0008\n",
    "\n",
    "#emotion prompt\n",
    "#'anger' 0.97, 'disgust' 0.97, 'fear' 0.97, 'guilt': 0.96, 'joy':0.95, 'sadness':0.97, 'shame' 0.97\n",
    "#0.0019   0.003    0.0017  0.004 0.0028  0.0009  0.006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4659e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "///\n",
      "2\n",
      "35\n",
      "15\n",
      "0\n",
      "26\n",
      "40\n",
      "///\n",
      "37\n",
      "465\n",
      "381\n",
      "14\n",
      "118\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "#'sadness', 'unhappy', 'grief', 'sorrow', 'loneliness', 'depression'\n",
    "#'joy', 'an achievement', 'pleasure', 'the awesome', 'happiness', 'the blessing'\n",
    "#'anger', 'annoyance', 'rage', 'outrage', 'fury', 'irritation'\n",
    "\n",
    "data=pd.read_csv('Isear_nli_syn_max_emotion_prompt.csv')\n",
    "#data=pd.read_csv('Isear_nli_syn_expr_emo_max.csv')\n",
    "sadness=0\n",
    "unhappy=0\n",
    "grief=0\n",
    "sorrow=0\n",
    "loneliness=0 \n",
    "depression=0\n",
    "\n",
    "joy=0\n",
    "achievement=0 \n",
    "pleasure=0 \n",
    "awsome=0\n",
    "happiness=0\n",
    "blessing=0\n",
    "\n",
    "anger=0\n",
    "annoyance=0\n",
    "rage=0\n",
    "outrage=0\n",
    "fury=0\n",
    "irritation=0\n",
    "\n",
    "disgust=0\n",
    "loathing=0\n",
    "bitterness=0\n",
    "ugliness=0\n",
    "repugnance=0\n",
    "revulsion=0\n",
    "#'fear', 'horror', 'anxiety', 'terror', 'dread', 'scare'\n",
    "fear=0\n",
    "horror=0\n",
    "anxiety=0\n",
    "terror=0\n",
    "dread=0\n",
    "scare=0\n",
    "\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['emotion'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_emotion'].tolist()\n",
    "emotion_word=data['emotion_word'].tolist()\n",
    "probs2=list()\n",
    "\n",
    "#'joy', 'an achievement', 'pleasure', 'the awesome', 'happiness', 'the blessing'\n",
    "\n",
    "#'disgust', 'loathing', 'bitterness', 'ugliness', 'repugnance', 'revulsion'\n",
    "#'shame', 'humiliation', 'embarrassment', 'disgrace', 'dishonor', 'discredit'\n",
    "#'guilt', 'culpability', 'responsibility', 'blameworthy', 'misconduct', 'regret'\n",
    "shame=0\n",
    "humiliation=0\n",
    "embarrassment=0\n",
    "disgrace=0\n",
    "dishonor=0\n",
    "discredit=0\n",
    "\n",
    "guilt=0\n",
    "culpability=0\n",
    "responsibility=0\n",
    "blameworthy=0\n",
    "misconduct=0\n",
    "regret=0\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i]!='sadness'and y_predict[i]=='sadness':\n",
    "        if emotion_word[i]=='sadness':\n",
    "            sadness+=1\n",
    "        if emotion_word[i]=='unhappiness':\n",
    "            unhappy+=1\n",
    "        if emotion_word[i]=='grief':\n",
    "            grief+=1\n",
    "        if emotion_word[i]=='sorrow':\n",
    "            sorrow+=1\n",
    "        if emotion_word[i]=='loneliness':\n",
    "            loneliness+=1\n",
    "        if emotion_word[i]=='depression':\n",
    "            depression+=1\n",
    "    if y_true[i] !='joy'and y_predict[i]=='joy':\n",
    "        if emotion_word[i]=='joy':\n",
    "            joy+=1\n",
    "        if emotion_word[i]=='an achievement':\n",
    "            achievement+=1\n",
    "        if emotion_word[i]=='pleasure':\n",
    "            pleasure+=1\n",
    "        if emotion_word[i]=='the awsome':\n",
    "            awsome+=1\n",
    "        if emotion_word[i]=='happiness':\n",
    "            happiness+=1\n",
    "        if emotion_word[i]=='the blessing':\n",
    "            blessing+=1\n",
    "\n",
    "#'anger', 'annoyance', 'rage', 'outrage', 'fury', 'irritation'\n",
    "    if y_true[i]!='anger'and y_predict[i]=='anger':\n",
    "        if emotion_word[i]=='anger':\n",
    "            anger+=1\n",
    "        if emotion_word[i]=='annoyance':\n",
    "            annoyance+=1\n",
    "        if emotion_word[i]=='rage':\n",
    "            rage+=1\n",
    "        if emotion_word[i]=='outrage':\n",
    "            outrage+=1\n",
    "        if emotion_word[i]=='fury':\n",
    "            fury+=1\n",
    "        if emotion_word[i]=='irritation':\n",
    "            irritation+=1\n",
    "\n",
    "#'disgust', 'loathing', 'bitterness', 'ugliness', 'repugnance', 'revulsion'\n",
    "\n",
    "    if y_true[i]!='disgust'and y_predict[i]=='disgust':\n",
    "        if emotion_word[i]=='disgust':\n",
    "            disgust+=1\n",
    "        if emotion_word[i]=='loathing':\n",
    "            loathing+=1\n",
    "        if emotion_word[i]=='bitterness':\n",
    "            bitterness+=1\n",
    "        if emotion_word[i]=='ugliness':\n",
    "            ugliness+=1\n",
    "        if emotion_word[i]=='repugnance':\n",
    "            repugnance+=1\n",
    "        if emotion_word[i]=='revulsion':\n",
    "            revulsion+=1\n",
    "\n",
    "#'fear', 'horror', 'anxiety', 'terror', 'dread', 'scare' \n",
    "\n",
    "\n",
    "    if y_true[i]!='fear'and y_predict[i]=='fear':\n",
    "        if emotion_word[i]=='fear':\n",
    "            fear+=1\n",
    "        if emotion_word[i]=='horror':\n",
    "            horror+=1\n",
    "        if emotion_word[i]=='anxiety':\n",
    "            anxiety+=1\n",
    "        if emotion_word[i]=='terror':\n",
    "            terror+=1\n",
    "        if emotion_word[i]=='dread':\n",
    "            dread+=1\n",
    "        if emotion_word[i]=='scare':\n",
    "            scare+=1\n",
    "            \n",
    "#'shame', 'humiliation', 'embarrassment', 'disgrace', 'dishonor', 'discredit'\n",
    "    if y_true[i]!='shame'and y_predict[i]=='shame':\n",
    "        if emotion_word[i]=='shame':\n",
    "            shame+=1\n",
    "        if emotion_word[i]=='humiliation':\n",
    "            humiliation+=1\n",
    "        if emotion_word[i]=='embarrassment':\n",
    "            embarrassment+=1\n",
    "        if emotion_word[i]=='disgrace':\n",
    "            disgrace+=1\n",
    "        if emotion_word[i]=='dishonor':\n",
    "            dishonor+=1\n",
    "        if emotion_word[i]=='discredit':\n",
    "            discredit+=1  \n",
    "#'guilt', 'culpability', 'responsibility', 'blameworthy', 'misconduct', 'regret' \n",
    "    if y_true[i]!='guilt'and y_predict[i]=='guilt':\n",
    "        if emotion_word[i]=='guilt':\n",
    "            guilt+=1\n",
    "        if emotion_word[i]=='culpability':\n",
    "            culpability+=1\n",
    "        if emotion_word[i]=='responsibility':\n",
    "            responsibility+=1\n",
    "        if emotion_word[i]=='blameworthy':\n",
    "            blameworthy+=1\n",
    "        if emotion_word[i]=='misconduct':\n",
    "            misconduct+=1\n",
    "        if emotion_word[i]=='regret':\n",
    "            regret+=1  \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#'fear', 'horror', 'anxiety', 'terror', 'dread', 'scare' \n",
    "\n",
    "\n",
    "##'shame', 'humiliation', 'embarrassment', 'disgrace', 'dishonor', 'discredit'\n",
    "print(\"///\")\n",
    "print(shame)\n",
    "print(humiliation)\n",
    "print(embarrassment)\n",
    "print(disgrace)\n",
    "print(dishonor)\n",
    "print(discredit)\n",
    "\n",
    "#'guilt', 'culpability', 'responsibility', 'blameworthy', 'misconduct', 'regret'\n",
    "print(\"///\")\n",
    "print(guilt)\n",
    "print(culpability)\n",
    "print(responsibility)\n",
    "print(blameworthy)\n",
    "print(misconduct)\n",
    "print(regret)\n",
    "\n",
    "#expr_s\n",
    "#y_true[i]==  and y_predict[i]==\n",
    "#'sadness' 251, 'unhappy' 393, 'grief' 162, 'sorrow' 36, 'loneliness' 28, 'depression' 0\n",
    "#y_true[i]!=  and y_predict[i]==\n",
    "#'sadness' 56, 'unhappy'  801, 'grief' 30, 'sorrow' 7, 'loneliness' 94, 'depression' 2\n",
    "\n",
    "#remove unhappy loneliness depression\n",
    "\n",
    "#y_true[i]==  and y_predict[i]==\n",
    "#'joy' 23, 'an achievement' 478, 'pleasure' 292, 'the awesome' 0 , 'happiness' 151, 'the blessing' 22\n",
    "\n",
    "#y_true[i]!=  and y_predict[i]==\n",
    "#'joy' 3, 'an achievement' 80, 'pleasure' 32, 'the awesome' 0, 'happiness' 6, 'the blessing' 8\n",
    "\n",
    "\n",
    "#y_true[i]==  and y_predict[i]==\n",
    "#'anger' 88, 'annoyance' 77, 'rage' 0, 'outrage' 1, 'fury' 0, 'irritation' 232\n",
    "#y_true[i]!=  and y_predict[i]==\n",
    "#'anger' 27, 'annoyance' 156, 'rage' 0, 'outrage' 0, 'fury' 0, 'irritation' 168\n",
    "\n",
    "#remove annoyance \n",
    "\n",
    "#y_true[i]==  and y_predict[i]==\n",
    "#'disgust' 31, 'loathing' 4, 'bitterness' 4, 'ugliness' 3, 'repugnance' 329, 'revulsion' 42\n",
    "#y_true[i]!=  and y_predict[i]==\n",
    "#'disgust' 0, 'loathing' 0, 'bitterness' 4, 'ugliness' 0, 'repugnance' 314, 'revulsion' 5\n",
    "\n",
    "\n",
    "\n",
    "#y_true[i]==  and y_predict[i]==\n",
    "\n",
    "#fear 244 horror 2  anxiety 156 terror 4 dread 75 scare 260  \n",
    "\n",
    "#y_true[i]!=  and y_predict[i]==\n",
    "#'fear' 51, 'horror' 7, 'anxiety' 95, 'terror' 2, 'dread' 16, 'scare' 69\n",
    "\n",
    "#remove horror \n",
    "\n",
    "#y_true[i]==  and y_predict[i]==\n",
    "#'shame' 0, 'humiliation' 30, 'embarrassment' 254, 'disgrace' 0, 'dishonor' 5 , 'discredit'11\n",
    "#y_true[i]!=  and y_predict[i]==\n",
    "#'shame' 2, 'humiliation' 18, 'embarrassment' 28, 'disgrace' 0, 'dishonor', 4 'discredit' 23\n",
    "\n",
    "#remove shame discredit\n",
    "\n",
    "#'guilt', 'culpability', 'responsibility', 'blameworthy', 'misconduct', 'regret'\n",
    "#true 129  124  23 193 159  108\n",
    "#false  16 117 81  313 348 82\n",
    "\n",
    "#remove responsibility, blameworthy misconduct \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747dc196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "///\n",
      "23\n",
      "303\n",
      "0\n",
      "88\n",
      "690\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "#'sadness', 'unhappy', 'grief', 'sorrow', 'loneliness', 'depression'\n",
    "#'joy', 'an achievement', 'pleasure', 'the awesome', 'happiness', 'the blessing'\n",
    "#'anger', 'annoyance', 'rage', 'outrage', 'fury', 'irritation'\n",
    "\n",
    "data=pd.read_csv('Isear_nli_syn_emotion_prompt_max_entailment_Deberta.csv')\n",
    "data=pd.read_csv('Isear_nli_syn_max_emo_s_prompt.csv')\n",
    "sadness=0\n",
    "unhappy=0\n",
    "grief=0\n",
    "sorrow=0\n",
    "loneliness=0 \n",
    "depression=0\n",
    "\n",
    "joy=0\n",
    "achievement=0 \n",
    "pleasure=0 \n",
    "awsome=0\n",
    "happiness=0\n",
    "blessing=0\n",
    "\n",
    "anger=0\n",
    "annoyance=0\n",
    "rage=0\n",
    "outrage=0\n",
    "fury=0\n",
    "irritation=0\n",
    "\n",
    "disgust=0\n",
    "loathing=0\n",
    "bitterness=0\n",
    "ugliness=0\n",
    "repugnance=0\n",
    "revulsion=0\n",
    "#'fear', 'horror', 'anxiety', 'terror', 'dread', 'scare'\n",
    "fear=0\n",
    "horror=0\n",
    "anxiety=0\n",
    "terror=0\n",
    "dread=0\n",
    "scare=0\n",
    "\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['emo_s'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_emo_s'].tolist()\n",
    "emotion_word=data['emotion_word'].tolist()\n",
    "probs2=list()\n",
    "\n",
    "#'joy', 'an achievement', 'pleasure', 'the awesome', 'happiness', 'the blessing'\n",
    "\n",
    "#'disgust', 'loathing', 'bitterness', 'ugliness', 'repugnance', 'revulsion'\n",
    "#'shame', 'humiliation', 'embarrassment', 'disgrace', 'dishonor', 'discredit'\n",
    "#'guilt', 'culpability', 'responsibility', 'blameworthy', 'misconduct', 'regret'\n",
    "shame=0\n",
    "humiliation=0\n",
    "embarrassment=0\n",
    "disgrace=0\n",
    "dishonor=0\n",
    "discredit=0\n",
    "\n",
    "guilt=0\n",
    "culpability=0\n",
    "responsibility=0\n",
    "blameworthy=0\n",
    "misconduct=0\n",
    "regret=0\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i]=='sadness'and y_predict[i]=='sadness':\n",
    "        if emotion_word[i]=='sadness':\n",
    "            sadness+=1\n",
    "        if emotion_word[i]=='unhappiness':\n",
    "            unhappy+=1\n",
    "        if emotion_word[i]=='grief':\n",
    "            grief+=1\n",
    "        if emotion_word[i]=='sorrow':\n",
    "            sorrow+=1\n",
    "        if emotion_word[i]=='loneliness':\n",
    "            loneliness+=1\n",
    "        if emotion_word[i]=='depression':\n",
    "            depression+=1\n",
    "#'joy', 'achievement', 'pleasure', 'awesome', 'happy', 'blessed'\n",
    "    if y_true[i]=='joy'and y_predict[i]=='joy':\n",
    "        if emotion_word[i]=='joy':\n",
    "            joy+=1\n",
    "        if emotion_word[i]=='an achievement':\n",
    "            achievement+=1\n",
    "        if emotion_word[i]=='pleasure':\n",
    "            pleasure+=1\n",
    "        if emotion_word[i]=='the awsome':\n",
    "            awsome+=1\n",
    "        if emotion_word[i]=='happiness':\n",
    "            happiness+=1\n",
    "        if emotion_word[i]=='the blessing':\n",
    "            blessing+=1\n",
    "\n",
    "#'anger', 'annoyance', 'rage', 'outrage', 'fury', 'irritation'\n",
    "    if y_true[i]=='anger'and y_predict[i]=='anger':\n",
    "        if emotion_word[i]=='anger':\n",
    "            anger+=1\n",
    "        if emotion_word[i]=='annoyance':\n",
    "            annoyance+=1\n",
    "        if emotion_word[i]=='rage':\n",
    "            rage+=1\n",
    "        if emotion_word[i]=='outrage':\n",
    "            outrage+=1\n",
    "        if emotion_word[i]=='fury':\n",
    "            fury+=1\n",
    "        if emotion_word[i]=='irritation':\n",
    "            irritation+=1\n",
    "\n",
    "#'disgust', 'loathing', 'bitterness', 'ugliness', 'repugnance', 'revulsion'\n",
    "\n",
    "    if y_true[i]=='disgust'and y_predict[i]=='disgust':\n",
    "        if emotion_word[i]=='disgust':\n",
    "            disgust+=1\n",
    "        if emotion_word[i]=='loathing':\n",
    "            loathing+=1\n",
    "        if emotion_word[i]=='bitterness':\n",
    "            bitterness+=1\n",
    "        if emotion_word[i]=='ugliness':\n",
    "            ugliness+=1\n",
    "        if emotion_word[i]=='repugnance':\n",
    "            repugnance+=1\n",
    "        if emotion_word[i]=='revulsion':\n",
    "            revulsion+=1\n",
    "\n",
    "#'fear', 'horror', 'anxiety', 'terror', 'dread', 'scare' \n",
    "\n",
    "\n",
    "    if y_true[i]!='fear'and y_predict[i]=='fear':\n",
    "        if emotion_word[i]=='fear':\n",
    "            fear+=1\n",
    "        if emotion_word[i]=='horror':\n",
    "            horror+=1\n",
    "        if emotion_word[i]=='anxiety':\n",
    "            anxiety+=1\n",
    "        if emotion_word[i]=='terror':\n",
    "            terror+=1\n",
    "        if emotion_word[i]=='dread':\n",
    "            dread+=1\n",
    "        if emotion_word[i]=='scare':\n",
    "            scare+=1\n",
    "            \n",
    "#'shame', 'humiliation', 'embarrassment', 'disgrace', 'dishonor', 'discredit'\n",
    "    if y_true[i]!='shame'and y_predict[i]=='shame':\n",
    "        if emotion_word[i]=='shame':\n",
    "            shame+=1\n",
    "        if emotion_word[i]=='humiliation':\n",
    "            humiliation+=1\n",
    "        if emotion_word[i]=='embarrassment':\n",
    "            embarrassment+=1\n",
    "        if emotion_word[i]=='disgrace':\n",
    "            disgrace+=1\n",
    "        if emotion_word[i]=='dishonor':\n",
    "            dishonor+=1\n",
    "        if emotion_word[i]=='discredit':\n",
    "            discredit+=1  \n",
    "#'guilt', 'culpability', 'responsibility', 'blameworthy', 'misconduct', 'regret' \n",
    "    if y_true[i]!='guilt'and y_predict[i]=='guilt':\n",
    "        if emotion_word[i]=='guilt':\n",
    "            guilt+=1\n",
    "        if emotion_word[i]=='culpability':\n",
    "            culpability+=1\n",
    "        if emotion_word[i]=='responsibility':\n",
    "            responsibility+=1\n",
    "        if emotion_word[i]=='blameworthy':\n",
    "            blameworthy+=1\n",
    "        if emotion_word[i]=='misconduct':\n",
    "            misconduct+=1\n",
    "        if emotion_word[i]=='regret':\n",
    "            regret+=1  \n",
    "            \n",
    "            \n",
    "\n",
    "#'guilt', 'culpability', 'responsibility', 'blameworthy', 'misconduct', 'regret'\n",
    "print(\"///\")\n",
    "print(guilt)\n",
    "print(culpability)\n",
    "print(responsibility)\n",
    "print(blameworthy)\n",
    "print(misconduct)\n",
    "print(regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78bc3aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "data = pd.read_csv('Isear_nli_syn_expr_emo_promp_max_edited.csv')\n",
    "\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'\n",
    "###\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['expr_s'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_expr_s'].tolist()\n",
    "prob_max=data['max_prob'].tolist()\n",
    "probs2=list()\n",
    "entropy_list=list()\n",
    "##\n",
    "labels2=list()\n",
    "expr_s2=list()\n",
    "text2=list()\n",
    "prob_max2=list()\n",
    "prob2=list()\n",
    "var_list=list()\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "#    if y_predict[i]!=y_true[i] and y_true[i]=='sadness':\n",
    "    m=probs[i]\n",
    "    m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "    string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "    float_list = [float(element) for element in string_elements]\n",
    "        \n",
    "#        e=entropy(float_list, base=2)\n",
    "        \n",
    "    var=statistics.variance(float_list)\n",
    "    var_list.append(var)\n",
    "#    entropy_list.append(e)\n",
    "        ##\n",
    "    labels2.append(y_true[i])\n",
    "    expr_s2.append(y_predict[i])\n",
    "    text2.append(text[i])\n",
    "    prob2.append(probs[i])\n",
    "    prob_max2.append(prob_max[i])\n",
    "\n",
    "data2={'labels':labels2, 'text':text2, 'expr_s':expr_s2, 'prob_expr_s':prob2, 'prob_max':prob_max2, 'variance':var_list}\n",
    "d=pd.DataFrame(data2)\n",
    "\n",
    "d.to_csv('Isear_nli_syn_max_expr_prompt2_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cf99b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "#Isear_nli_syn_expr_emo_promp_max_edited\n",
    "\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'\n",
    "\n",
    "data = pd.read_csv('Isear_nli_syn_expr_emo_max.csv')\n",
    "\n",
    "###\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['expr_s'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_expr_s'].tolist()\n",
    "prob_max=data['prob_max'].tolist()\n",
    "emotion_word= data['emotion_word'].tolist()\n",
    "probs2=list()\n",
    "entropy_list=list()\n",
    "##\n",
    "labels2=list()\n",
    "expr_s2=list()\n",
    "text2=list()\n",
    "prob_max2=list()\n",
    "prob2=list()\n",
    "var_list=list()\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "#    if y_predict[i]!=y_true[i] and y_true[i]=='sadness':\n",
    "    m=probs[i]\n",
    "    m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "    string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "    float_list = [float(element) for element in string_elements]\n",
    "        \n",
    "#        e=entropy(float_list, base=2)\n",
    "        \n",
    "    var=statistics.variance(float_list)\n",
    "    var_list.append(var)\n",
    "#    entropy_list.append(e)\n",
    "        ##\n",
    "    labels2.append(y_true[i])\n",
    "    expr_s2.append(y_predict[i])\n",
    "    text2.append(text[i])\n",
    "    prob2.append(probs[i])\n",
    "    prob_max2.append(prob_max[i])\n",
    "\n",
    "data2={'labels':labels2, 'text':text2, 'expr_s':expr_s2, 'prob_expr_s':prob2, 'emotion_word':emotion_word, 'prob_max':prob_max2, 'variance':var_list}\n",
    "d=pd.DataFrame(data2)\n",
    "\n",
    "d.to_csv('Isear_nli_syn_max_expr_emo2_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8feeca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "#Isear_nli_syn_expr_emo_promp_max_edited\n",
    "\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'\n",
    "\n",
    "data = pd.read_csv('Isear_nli_syn_max_emo_s_prompt.csv')\n",
    "\n",
    "###\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['emo_s'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_emo_s'].tolist()\n",
    "prob_max=data['prob_max'].tolist()\n",
    "emotion_word= data['emotion_word'].tolist()\n",
    "probs2=list()\n",
    "entropy_list=list()\n",
    "##\n",
    "labels2=list()\n",
    "expr_s2=list()\n",
    "text2=list()\n",
    "prob_max2=list()\n",
    "prob2=list()\n",
    "var_list=list()\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "#    if y_predict[i]!=y_true[i] and y_true[i]=='sadness':\n",
    "    m=probs[i]\n",
    "    m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "    string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "    float_list = [float(element) for element in string_elements]\n",
    "        \n",
    "#        e=entropy(float_list, base=2)\n",
    "        \n",
    "    var=statistics.variance(float_list)\n",
    "    var_list.append(var)\n",
    "#    entropy_list.append(e)\n",
    "        ##\n",
    "    labels2.append(y_true[i])\n",
    "    expr_s2.append(y_predict[i])\n",
    "    text2.append(text[i])\n",
    "    prob2.append(probs[i])\n",
    "    prob_max2.append(prob_max[i])\n",
    "\n",
    "data2={'labels':labels2, 'text':text2, 'emo_s':expr_s2, 'prob_emo_s':prob2, 'emotion_word':emotion_word, 'prob_max':prob_max2, 'variance':var_list}\n",
    "d=pd.DataFrame(data2)\n",
    "\n",
    "d.to_csv('Isear_nli_syn_max_emo_s_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1c2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66ea9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "data = pd.read_csv('Isear_nli_syn_max_emo_s_prompt.csv')\n",
    "#Isear_nli_syn_expr_emo_max.csv\n",
    "#Isear_nli_syn_max_emo_s_prompt.csv\n",
    "###\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['emo_s'].tolist()\n",
    "text=data['text'].tolist()\n",
    "probs=data['prob_emo_s'].tolist()\n",
    "prob_max=data['prob_max'].tolist()\n",
    "emotion_word=data['emotion_word'].tolist()\n",
    "probs2=list()\n",
    "entropy_list=list()\n",
    "##\n",
    "labels2=list()\n",
    "expr_s2=list()\n",
    "text2=list()\n",
    "prob_max2=list()\n",
    "prob2=list()\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    \n",
    "    m=probs[i]\n",
    "    m = m.strip(\"[]\")\n",
    "\n",
    "# Step 2: Split the string by commas\n",
    "    string_elements = m.split(\",\")\n",
    "\n",
    "# Step 3: Convert each element to a float\n",
    "    float_list = [float(element) for element in string_elements]\n",
    "    float_list2=[(i)/max(float_list) for i in float_list]\n",
    "    \n",
    "        \n",
    "    e=entropy(float_list2, base=2)\n",
    "    \n",
    "    entropy_list.append(e)\n",
    "        ##\n",
    "    labels2.append(y_true[i])\n",
    "    expr_s2.append(y_predict[i])\n",
    "    text2.append(text[i])\n",
    "    prob2.append(probs[i])\n",
    "    prob_max2.append(prob_max[i])\n",
    "\n",
    "data2={'labels':labels2, 'text':text2, 'emo_s':expr_s2, 'prob_emo_s':prob2, 'emotion_word':emotion_word, 'prob_max':prob_max2, 'entropy':entropy_list}\n",
    "d=pd.DataFrame(data2)\n",
    "\n",
    "d.to_csv('Isear_nli_syn_emo_s_max_prompt3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a25761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9778315336393557\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame' \n",
    "#data = pd.read_csv('ISEAR-nli-prompt-emotion3-deberta.csv')\n",
    "#ISEAR_nli_Deberta_prompt2.csv\n",
    "#ISEAR-nli-prompt-emotion-roberta.csv\n",
    "#TEC_nli-deberta-feels-emo.csv\n",
    "#data = pd.read_csv('TEC_nli_prompt-emotion-prediction_file_deberta2.csv')\n",
    "#Isear_nli_syn_avg_expr_prompt2\n",
    "\n",
    "#Isear_nli_syn_emo_s_max_prompt3\n",
    "data = pd.read_csv('Isear_nli_syn_max_emo_s_prompt.csv')\n",
    "#\n",
    "#data = pd.read_csv('Isear_nli_syn_expr_max_prompt3.csv')\n",
    "dict_emo={'anger':0, 'disgust':1, 'fear':2, 'guilt':3, 'joy':4, 'sadness':5, 'shame':6 }\n",
    "#Isear_nli_syn_avg_emo_s_prompt2\n",
    "###\n",
    "y_true = data['labels'].tolist()\n",
    "y_predict=data['emo_s'].tolist()\n",
    "probs=data['prob_emo_s'].tolist()\n",
    "y_predict2=list()\n",
    "probs2=list()\n",
    "emotions=list()\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if  y_predict[i]!='disgust' and y_true[i]=='disgust':\n",
    "        y_predict2.append(dict_emo[y_predict[i]])\n",
    "        \n",
    "class_counts=np.bincount(y_predict2)\n",
    "probs=class_counts/len(y_predict2)\n",
    "entropy_emotions=entropy(probs, base=2)\n",
    "print(entropy_emotions)\n",
    "\n",
    "#emo_s\n",
    "#'anger' 1.94, 'disgust' 1.97 , 'fear' 2.11 , 'guilt' 2.15, 'joy' 1.91, 'sadness' 2.2, 'shame' 2.02\n",
    "\n",
    "#expr_s\n",
    "##'anger': 1.91, 'disgust':2.19, 'fear':2.05, 'guilt':2.16, 'joy':1.99, 'sadness' 2.34, 'shame' 2.15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab04074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
